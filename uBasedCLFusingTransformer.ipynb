{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "from torch.optim import Adam, AdamW, SGD\n",
    "from pytorch_lightning import trainer\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(pl.LightningModule):\n",
    "    def __init__(self,  hyper_parameter: dict):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.DATA_SET = hyper_parameter[\"data_path\"]\n",
    "\n",
    "        ### 하이퍼파라미터 ###\n",
    "        self.MAX_LENGTH = hyper_parameter[\"max_length\"] if (\"max_length\" in hyper_parameter) else 150\n",
    "        self.LEARNING_RATE = hyper_parameter[\"lr\"] if (\"lr\" in hyper_parameter) else 5e-6\n",
    "        self.EPOCHS = hyper_parameter[\"epochs\"] if (\"epochs\" in hyper_parameter) else 5\n",
    "        self.OPTIMIZER = hyper_parameter[\"optimizer\"] if (\"optimizer\" in hyper_parameter) else \"adamw\"\n",
    "        self.GAMMA = hyper_parameter[\"gamma\"] if (\"gamma\" in hyper_parameter) else 0.5\n",
    "        self.BATCH_SIZE = hyper_parameter[\"batch_size\"] if (\"batch_size\" in hyper_parameter) else 32\n",
    "\n",
    "\n",
    "        self.model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\")\n",
    "        self.model.classifier = torch.nn.Linear(self.model.config.hidden_size, 2) #출력 변경\n",
    "        #self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "        self.tokenizer = Tokenizer(char_level=True)\n",
    "        # self.tokenizer.add_special_tokens({\n",
    "        #     \"cls_token\": \"[CLS]\", \"sep_token\": \"[SEP]\", \"unk_token\": \"<unk>\",\n",
    "        #     \"pad_token\": \"<pad>\", \"mask_token\": \"[MASK]\"})\n",
    "        self.loss = torch.nn.BCEWithLogitsLoss() #이진분류를 위한 손실 함수 수정\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = self.model(**kwargs)\n",
    "        return output[0]\n",
    "\n",
    "    def __step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        output = self.forward(input_ids=data)\n",
    "        logits = output[:,1]\n",
    "        loss = self.loss(logits.unsqueeze(-1), labels.unsqueeze(-1))\n",
    "\n",
    "        preds = output.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__step(batch, batch_idx)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.__step(batch, batch_idx)\n",
    "\n",
    "    def __epoch_end(self, outputs, state=\"train\"):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for i in outputs:\n",
    "            loss += i[\"loss\"].cpu().detach()\n",
    "            y_true += i[\"y_true\"]\n",
    "            y_pred += i[\"y_pred\"]\n",
    "\n",
    "        loss = loss / len(outputs)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, labels=np.unique(y_pred), zero_division=1)\n",
    "        rec = recall_score(y_true, y_pred, labels=np.unique(y_pred), zero_division=1)\n",
    "        f1 = f1_score(y_true, y_pred, labels=np.unique(y_pred), zero_division=1)\n",
    "\n",
    "        print(f\"[Epoch {self.trainer.current_epoch} {state.upper()}]\",\n",
    "              f\"Loss={loss}, Acc={acc}, Prec={prec}, Rec={rec}, F1={f1},\",\n",
    "              \"CM={}\".format(str(cm).replace(\"\\n\", \"\")))\n",
    "\n",
    "        return {\"loss\": loss, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.__epoch_end(outputs, state=\"train\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.__epoch_end(outputs, state=\"val\")\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        self.__epoch_end(outputs, state=\"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.OPTIMIZER == \"adam\":\n",
    "            optimizer = Adam(self.parameters(), lr=self.LEARNING_RATE)\n",
    "        elif self.OPTIMIZER == \"adamw\":\n",
    "            optimizer = AdamW(self.parameters(), lr=self.LEARNING_RATE)\n",
    "        elif self.OPTIMIZER == \"sgd\":\n",
    "            optimizer = SGD(self.parameters(), lr=self.LEARNING_RATE)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"'{self.OPTIMIZER}' is not available.\")\n",
    "\n",
    "        scheduler = ExponentialLR(optimizer, gamma=self.GAMMA)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"scheduler\": scheduler\n",
    "        }\n",
    "    \n",
    "    def encode_data(self, text):\n",
    "        encoded_text = self.tokenizer.texts_to_sequences(text)\n",
    "        pad_text = pad_sequences(encoded_text, maxlen=self.MAX_LENGTH, padding='post')\n",
    "        return pad_text\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Prepare your dataset here\n",
    "\n",
    "        df = pd.read_csv(self.DATA_SET)\n",
    "\n",
    "        df['phishing'] = (df['status'] == 'phishing')\n",
    "        df.drop('status', inplace=True, axis=1)\n",
    "\n",
    "        df_URL=df[['url', 'length_url', 'phishing', 'shortening_service']]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df_URL[['url', 'shortening_service']], \n",
    "                                                            df_URL['phishing'], \n",
    "                                                            test_size = 0.25, \n",
    "                                                            random_state = 32)\n",
    "        \n",
    "        X_test = X_test['url']\n",
    "\n",
    "        X_train=X_train[X_train['shortening_service']==0]\n",
    "        y_train = y_train.loc[X_train.index]\n",
    "        X_train = X_train['url']\n",
    "\n",
    "\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                            y_train, \n",
    "                                                            test_size = 0.1)\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        train_texts = X_train\n",
    "        train_labels = y_train\n",
    "        val_texts = X_val\n",
    "        val_labels = y_val\n",
    "        test_texts = X_test\n",
    "        test_labels = y_test\n",
    "\n",
    "        train_inputs = self.encode_data(train_texts)\n",
    "        val_inputs = self.encode_data(val_texts)\n",
    "        test_inputs = self.encode_data(test_texts)\n",
    "\n",
    "        self.train_dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.long), \n",
    "                                           torch.tensor(train_labels.to_list(), dtype=torch.float))\n",
    "        self.val_dataset = TensorDataset(torch.tensor(val_inputs, dtype=torch.long), \n",
    "                                         torch.tensor(val_labels.to_list(), dtype=torch.float))\n",
    "        self.test_dataset = TensorDataset(torch.tensor(test_inputs, dtype=torch.long), \n",
    "                                          torch.tensor(test_labels.to_list(), dtype=torch.float))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.BATCH_SIZE, shuffle=False, num_workers=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.BATCH_SIZE, shuffle=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameter = {\n",
    "    'lr': 5e-6,  # Starting Learning Rate\n",
    "    'epochs' : 10,\n",
    "    'optimizer' : 'adamw',\n",
    "    'gamma' : 0.9,\n",
    "    'max_length': 400,\n",
    "    'batch_size': 64,\n",
    "    'data_path' : 'shortURL/dataset_phishing.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel(hyper_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename='epoch{epoch}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.trainer.Trainer(\n",
    "    #callbacks=[checkpoint_callback],\n",
    "    max_epochs=1,\n",
    "    deterministic=torch.cuda.is_available(),\n",
    "    accelerator='gpu' if torch.cuda.is_available() else None, \n",
    "    devices=[0]\n",
    "    #tpu_cores=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.resize_token_embeddings(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel(hyper_parameter)\n",
    "model = torch.load(\"shortURL/URL_classification0.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "\n",
    "trainer.test(model)\n",
    "\n",
    "end_time = time.process_time()\n",
    "print(f\"time elapsed : {int(round((end_time - start_time) * 1000))}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"shortURL/URL_classification1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(model.DATA_SET)\n",
    "\n",
    "df['phishing'] = (df['status'] == 'phishing')\n",
    "df.drop('status', inplace=True, axis=1)\n",
    "\n",
    "df_URL=df[['url', 'length_url', 'phishing', 'shortening_service']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_URL[['url', 'shortening_service']], \n",
    "                                                            df_URL['phishing'], \n",
    "                                                            test_size = 0.25, \n",
    "                                                            random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test[X_test['shortening_service']==0]\n",
    "y_test = y_test.loc[X_test.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_input=model.encode_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(torch.tensor(X_test_input[:1], dtype=torch.long), \n",
    "                                    torch.tensor(y_test[:1].to_list(), dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_time = time.process_time()\n",
    "\n",
    "trainer.test(model, dataloaders=DataLoader(test_dataset, shuffle=False, num_workers=32))\n",
    "\n",
    "end_time = time.process_time()\n",
    "print(f\"time elapsed : {int(round((end_time - start_time) * 1000))}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(x):\n",
    "    print(model(input_ids=x).argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_input[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.process_time()\n",
    "\n",
    "infer(torch.tensor(X_test_input[:1]))\n",
    "\n",
    "end_time = time.process_time()\n",
    "print(f\"time elapsed : {int(round((end_time - start_time) * 1000))}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
